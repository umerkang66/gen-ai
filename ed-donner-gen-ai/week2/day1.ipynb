{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "gemini = genai.Client(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she just couldn’t handle his standard deviation!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because they couldn't find any significant correlation!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\", messages=prompts, temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist go broke?\n",
      "\n",
      "Because she kept stacking her files!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(model=\"gpt-4.1-nano\", messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the logistic regression model?\n",
      "\n",
      "Because it just couldn’t commit!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "completion = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1\", messages=prompts, temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many data scientists does it take to change a lightbulb?  \n",
      "None—they’d rather run an A/B test to see if users actually prefer light over darkness!\n"
     ]
    }
   ],
   "source": [
    "# If you have access to this, here is the reasoning model o4-mini\n",
    "# This is trained to think through its response before replying\n",
    "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
    "\n",
    "completion = openai.chat.completions.create(model=\"o4-mini\", messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, you got it! Here's one for ya:\n",
      "\n",
      "Why did the AI break up with the calculator?\n",
      "Because she said he was too \"add-ictive\"!\n",
      "\n",
      "And speaking of smart decisions, how AI works in a few words:\n",
      "**It's computers learning from data to spot patterns and make smart decisions.**\n"
     ]
    }
   ],
   "source": [
    "response = gemini.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=system_message + \"\\n Explain how AI works in a few words.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't data scientists like to play hide-and-seek?\n",
      "\n",
      "...Because good luck finding anything after you've dropped all the null values!\n"
     ]
    }
   ],
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\", messages=prompts\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that responds in Markdown\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# How to Decide if a Business Problem is Suitable for an LLM Solution\n",
       "\n",
       "Large Language Models (LLMs) like GPT-4 are powerful tools, but they are not a one-size-fits-all solution. To determine if a business problem is suitable for an LLM solution, consider the following criteria:\n",
       "\n",
       "## 1. Nature of the Problem\n",
       "- **Text-Centric Tasks:** LLMs excel at tasks involving natural language such as:\n",
       "  - Summarization\n",
       "  - Translation\n",
       "  - Text generation\n",
       "  - Sentiment analysis\n",
       "  - Question answering\n",
       "  - Chatbots and conversational agents\n",
       "- **Complex Reasoning or Structured Data:** LLMs may struggle with problems requiring precise numerical calculations, structured data processing, or domain-specific knowledge without fine-tuning.\n",
       "\n",
       "## 2. Data Availability\n",
       "- LLMs require large amounts of textual data (often unstructured) either for fine-tuning or prompt engineering.\n",
       "- If your problem involves proprietary or sensitive data, consider data privacy and compliance issues.\n",
       "\n",
       "## 3. Output Requirements\n",
       "- **Flexible, Natural Language Outputs:** If your solution requires generating or interpreting human-like text, LLMs are suitable.\n",
       "- **High Accuracy and Deterministic Outputs:** For tasks requiring exact answers (e.g., legal contracts, medical diagnosis), LLMs should be used cautiously or supplemented with domain-specific validation.\n",
       "\n",
       "## 4. Integration Complexity\n",
       "- Can the problem solution be integrated into existing workflows via APIs or applications that utilize LLMs?\n",
       "- Are stakeholders comfortable with the probabilistic nature of LLM outputs?\n",
       "\n",
       "## 5. Cost and Performance\n",
       "- LLM inference can be costly and latency-sensitive.\n",
       "- Evaluate if the business benefits justify the cost of deployment and maintenance.\n",
       "\n",
       "## 6. Ethical and Compliance Considerations\n",
       "- Consider bias, fairness, and ethical implications.\n",
       "- Ensure compliance with regulations related to AI and data usage.\n",
       "\n",
       "---\n",
       "\n",
       "## Summary Checklist\n",
       "\n",
       "| Criteria                     | Suitable if…                                      |\n",
       "|------------------------------|-------------------------------------------------|\n",
       "| Text-centric task             | Problem involves understanding/generating text  |\n",
       "| Data availability             | Sufficient relevant textual data available       |\n",
       "| Output nature                 | Flexible, natural language outputs acceptable    |\n",
       "| Accuracy requirement          | Some tolerance for probabilistic answers         |\n",
       "| Integration feasibility       | Can be integrated via APIs or apps                |\n",
       "| Cost-benefit analysis         | Business value outweighs cost and complexity      |\n",
       "| Ethical/compliance concerns   | Risks are manageable and compliant                 |\n",
       "\n",
       "---\n",
       "\n",
       "## Additional Tips\n",
       "- Start with a **proof of concept** to evaluate effectiveness.\n",
       "- Combine LLMs with other AI or rule-based systems for better accuracy.\n",
       "- Use prompt engineering to tailor LLM outputs to your needs before considering fine-tuning.\n",
       "\n",
       "---\n",
       "\n",
       "If your business problem aligns well with these considerations, an LLM-based solution is worth exploring.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\", messages=prompts, temperature=0.7, stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or \"\"\n",
    "    reply = reply.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
